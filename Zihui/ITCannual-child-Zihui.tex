\subsection{大規模言語モデルの研究とその応用(Li Zihui）}

Educational Large Language Models (LLMs): This research area focuses on enhancing the discovery of educational resources and the interpretability of transformer models in NLP education. We explored whether traditional methods could effectively identify high-quality study materials and proposed a transfer learning-based pipeline to improve resource discovery, especially for generating introductory paragraphs in educational content \cite{ireneli-3,ireneli-9}. Additionally, we assessed the potential of LLMs as tools for learning support, presenting a benchmark to evaluate their performance in various NLP tasks \cite{ireneli-5}. Our investigation also included the generation of concise survey articles in the NLP domain using LLMs. While GPT-created surveys were found to be more up-to-date and accessible than human-written ones, some limitations, such as occasional factual inaccuracies, were noted \cite{ireneli-11}. Furthermore, we explored the use of LLMs in teaching complex legal concepts through narrative methods \cite{ireneli-8}. Our LLM research works \cite{ireneli-1} have been noticed by \textbf{Nature News}, and I had the opportunity to be interviewed by a reporter, where I shared my insights on the impact of Large Language Models (LLMs) from the academia perspective \cite{ireneli-13}.

Medical Large Language Models (LLMs): In medical LLM research, we focus on enhancing LLMs' capabilities in medical question answering and teaching complex legal concepts via storytelling. Collaborating with MatsuoLab, we developed a framework combining knowledge graphs and ranking techniques to boost LLMs' effectiveness in medical queries \cite{ireneli-7}. We also introduced Ascle, a Python NLP toolkit for medical text generation \cite{ireneli-10}.

Other Benchmarks for NLP Open Questions: Our research extends to improving knowledge graph completion, evaluating LLMs in NLP problem-solving, and developing medical text generation tools. We investigated the enhancement of knowledge graph completion using node neighborhood data \cite{ireneli-4} and explored methods to better interpret transformer models by emphasizing crucial information \cite{ireneli-6}. Besides, we have been investigating other branches including graph methods for news encoding \cite{ireneli-2,ireneli-12}.


