\subsection{Research on efficient transformers and mining knowledge from unstructured data（Li Zihui）}

% The first research branch is on efficient transformers. Efficient Transformers are popular for long sequence modeling due to their subquadratic memory and time complexity. However, Sparse Transformer, which improves efficiency by restricting self-attention to predefined sparse patterns, may compromise the expressiveness of the Transformer when important token correlations are distant. To overcome this limitation, we propose Diffuser, an efficient Transformer that incorporates all token interactions within one attention layer while maintaining low computation and memory costs. Diffuser leverages Attention Diffusion to expand the receptive field of sparse attention, allowing it to compute multi-hop token correlations based on all paths between corresponding disconnected tokens. We demonstrate the expressiveness of Diffuser as a universal sequence approximator and its ability to approximate full-attention using a graph expander property. Evaluations show that Diffuser outperforms state-of-the-art benchmarks in both expressiveness and efficiency for language modeling, image modeling, and Long Range Arena (LRA). We also propose a new method that improves the complexity of masked attention from $O(n^2)$ to $O(n)$ by decomposing it into local and global attention. Overall, Diffuser is a promising approach for long sequence modeling that combines the benefits of sparse attention and full-attention. 
% \cite{feng2022diffuser}

% Another research trial proposes a node neighborhood-enhanced framework for knowledge graph completion that enriches the head node information by modeling the head entity neighborhood from multiple hops using graph neural networks. The model also includes an additional edge link prediction task to improve KGC, which is evaluated on two public datasets and demonstrated to be simple yet effective. \cite{li2023nnkgc}

% Lastly, a survey paper reviews the application of deep learning methods for Natural Language Processing (NLP) on electronic health records (EHRs). Recent advances in neural network and deep learning techniques have shown promising results in improving EHR analysis and outperforming traditional statistical and rule-based systems. The survey summarizes various neural NLP methods for EHR applications, including classification, prediction, word embeddings, extraction, generation, question answering, phenotyping, knowledge graphs, medical dialogue, multilinguality, and interpretability. \cite{li2022neural}
